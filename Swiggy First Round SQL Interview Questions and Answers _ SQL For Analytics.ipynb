{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d0c2b80-8c31-4d93-b1c8-9d984a02081b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>orderid</th><th>custid</th><th>city</th><th>del_partner</th><th>order_time</th><th>deliver_time</th><th>predicted_time</th></tr></thead><tbody><tr><td>1</td><td>101</td><td>Mumbai</td><td>Partner A</td><td>2024-12-18T10:00:00Z</td><td>2024-12-18T11:30:00Z</td><td>60</td></tr><tr><td>2</td><td>102</td><td>Delhi</td><td>Partner A</td><td>2024-12-18T09:00:00Z</td><td>2024-12-18T10:00:00Z</td><td>45</td></tr><tr><td>3</td><td>103</td><td>Pune</td><td>Partner A</td><td>2024-12-18T15:00:00Z</td><td>2024-12-18T15:30:00Z</td><td>30</td></tr><tr><td>4</td><td>104</td><td>Mumbai</td><td>Partner A</td><td>2024-12-18T14:00:00Z</td><td>2024-12-18T14:50:00Z</td><td>45</td></tr><tr><td>5</td><td>105</td><td>Bangalore</td><td>Partner B</td><td>2024-12-18T08:00:00Z</td><td>2024-12-18T08:29:00Z</td><td>30</td></tr><tr><td>6</td><td>106</td><td>Hyderabad</td><td>Partner B</td><td>2024-12-18T13:00:00Z</td><td>2024-12-18T14:00:00Z</td><td>70</td></tr><tr><td>7</td><td>107</td><td>Kolkata</td><td>Partner B</td><td>2024-12-18T10:00:00Z</td><td>2024-12-18T10:40:00Z</td><td>45</td></tr><tr><td>8</td><td>108</td><td>Delhi</td><td>Partner B</td><td>2024-12-18T18:00:00Z</td><td>2024-12-18T18:30:00Z</td><td>40</td></tr><tr><td>9</td><td>109</td><td>Chennai</td><td>Partner C</td><td>2024-12-18T07:00:00Z</td><td>2024-12-18T07:40:00Z</td><td>30</td></tr><tr><td>10</td><td>110</td><td>Mumbai</td><td>Partner C</td><td>2024-12-18T12:00:00Z</td><td>2024-12-18T13:00:00Z</td><td>50</td></tr><tr><td>11</td><td>111</td><td>Delhi</td><td>Partner C</td><td>2024-12-18T09:00:00Z</td><td>2024-12-18T09:35:00Z</td><td>30</td></tr><tr><td>12</td><td>112</td><td>Hyderabad</td><td>Partner C</td><td>2024-12-18T16:00:00Z</td><td>2024-12-18T16:45:00Z</td><td>30</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         101,
         "Mumbai",
         "Partner A",
         "2024-12-18T10:00:00Z",
         "2024-12-18T11:30:00Z",
         60
        ],
        [
         2,
         102,
         "Delhi",
         "Partner A",
         "2024-12-18T09:00:00Z",
         "2024-12-18T10:00:00Z",
         45
        ],
        [
         3,
         103,
         "Pune",
         "Partner A",
         "2024-12-18T15:00:00Z",
         "2024-12-18T15:30:00Z",
         30
        ],
        [
         4,
         104,
         "Mumbai",
         "Partner A",
         "2024-12-18T14:00:00Z",
         "2024-12-18T14:50:00Z",
         45
        ],
        [
         5,
         105,
         "Bangalore",
         "Partner B",
         "2024-12-18T08:00:00Z",
         "2024-12-18T08:29:00Z",
         30
        ],
        [
         6,
         106,
         "Hyderabad",
         "Partner B",
         "2024-12-18T13:00:00Z",
         "2024-12-18T14:00:00Z",
         70
        ],
        [
         7,
         107,
         "Kolkata",
         "Partner B",
         "2024-12-18T10:00:00Z",
         "2024-12-18T10:40:00Z",
         45
        ],
        [
         8,
         108,
         "Delhi",
         "Partner B",
         "2024-12-18T18:00:00Z",
         "2024-12-18T18:30:00Z",
         40
        ],
        [
         9,
         109,
         "Chennai",
         "Partner C",
         "2024-12-18T07:00:00Z",
         "2024-12-18T07:40:00Z",
         30
        ],
        [
         10,
         110,
         "Mumbai",
         "Partner C",
         "2024-12-18T12:00:00Z",
         "2024-12-18T13:00:00Z",
         50
        ],
        [
         11,
         111,
         "Delhi",
         "Partner C",
         "2024-12-18T09:00:00Z",
         "2024-12-18T09:35:00Z",
         30
        ],
        [
         12,
         112,
         "Hyderabad",
         "Partner C",
         "2024-12-18T16:00:00Z",
         "2024-12-18T16:45:00Z",
         30
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "orderid",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "custid",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "del_partner",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "deliver_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "predicted_time",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Schema for swiggy_orders\n",
    "swiggy_orders_schema = StructType([\n",
    "    StructField(\"orderid\", IntegerType(), True),\n",
    "    StructField(\"custid\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"del_partner\", StringType(), True),\n",
    "    StructField(\"order_time\", TimestampType(), True),\n",
    "    StructField(\"deliver_time\", TimestampType(), True),\n",
    "    StructField(\"predicted_time\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for swiggy_orders (Convert string timestamps to datetime)\n",
    "swiggy_orders_data = [\n",
    "    (1, 101, 'Mumbai', 'Partner A', datetime.strptime('2024-12-18 10:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 11:30:00', '%Y-%m-%d %H:%M:%S'), 60),\n",
    "    (2, 102, 'Delhi', 'Partner A', datetime.strptime('2024-12-18 09:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 10:00:00', '%Y-%m-%d %H:%M:%S'), 45),\n",
    "    (3, 103, 'Pune', 'Partner A', datetime.strptime('2024-12-18 15:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 15:30:00', '%Y-%m-%d %H:%M:%S'), 30),\n",
    "    (4, 104, 'Mumbai', 'Partner A', datetime.strptime('2024-12-18 14:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 14:50:00', '%Y-%m-%d %H:%M:%S'), 45),\n",
    "    (5, 105, 'Bangalore', 'Partner B', datetime.strptime('2024-12-18 08:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 08:29:00', '%Y-%m-%d %H:%M:%S'), 30),\n",
    "    (6, 106, 'Hyderabad', 'Partner B', datetime.strptime('2024-12-18 13:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 14:00:00', '%Y-%m-%d %H:%M:%S'), 70),\n",
    "    (7, 107, 'Kolkata', 'Partner B', datetime.strptime('2024-12-18 10:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 10:40:00', '%Y-%m-%d %H:%M:%S'), 45),\n",
    "    (8, 108, 'Delhi', 'Partner B', datetime.strptime('2024-12-18 18:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 18:30:00', '%Y-%m-%d %H:%M:%S'), 40),\n",
    "    (9, 109, 'Chennai', 'Partner C', datetime.strptime('2024-12-18 07:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 07:40:00', '%Y-%m-%d %H:%M:%S'), 30),\n",
    "    (10, 110, 'Mumbai', 'Partner C', datetime.strptime('2024-12-18 12:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 13:00:00', '%Y-%m-%d %H:%M:%S'), 50),\n",
    "    (11, 111, 'Delhi', 'Partner C', datetime.strptime('2024-12-18 09:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 09:35:00', '%Y-%m-%d %H:%M:%S'), 30),\n",
    "    (12, 112, 'Hyderabad', 'Partner C', datetime.strptime('2024-12-18 16:00:00', '%Y-%m-%d %H:%M:%S'),\n",
    "     datetime.strptime('2024-12-18 16:45:00', '%Y-%m-%d %H:%M:%S'), 30)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "swiggy_orders_df = spark.createDataFrame(swiggy_orders_data, schema=swiggy_orders_schema)\n",
    "swiggy_orders_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a296134-311d-4d11-b7d2-517295238859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_date</th><th>customer_id</th><th>store_id</th><th>product_id</th><th>sale</th><th>order_value</th></tr></thead><tbody><tr><td>2024-12-01</td><td>109</td><td>1</td><td>3</td><td>2</td><td>700</td></tr><tr><td>2024-12-02</td><td>110</td><td>2</td><td>2</td><td>1</td><td>300</td></tr><tr><td>2024-12-03</td><td>111</td><td>1</td><td>5</td><td>3</td><td>900</td></tr><tr><td>2024-12-04</td><td>112</td><td>3</td><td>1</td><td>2</td><td>500</td></tr><tr><td>2024-12-05</td><td>113</td><td>3</td><td>4</td><td>4</td><td>1200</td></tr><tr><td>2024-12-05</td><td>114</td><td>3</td><td>4</td><td>2</td><td>400</td></tr><tr><td>2024-12-05</td><td>115</td><td>3</td><td>4</td><td>1</td><td>300</td></tr><tr><td>2024-12-01</td><td>101</td><td>1</td><td>4</td><td>2</td><td>500</td></tr><tr><td>2024-12-01</td><td>102</td><td>1</td><td>4</td><td>1</td><td>300</td></tr><tr><td>2024-12-02</td><td>103</td><td>2</td><td>4</td><td>3</td><td>900</td></tr><tr><td>2024-12-02</td><td>104</td><td>2</td><td>4</td><td>1</td><td>400</td></tr><tr><td>2024-12-03</td><td>105</td><td>1</td><td>4</td><td>2</td><td>600</td></tr><tr><td>2024-12-03</td><td>106</td><td>1</td><td>4</td><td>3</td><td>800</td></tr><tr><td>2024-12-04</td><td>107</td><td>3</td><td>4</td><td>1</td><td>200</td></tr><tr><td>2024-12-04</td><td>108</td><td>3</td><td>4</td><td>2</td><td>500</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-12-01",
         109,
         1,
         3,
         2,
         700
        ],
        [
         "2024-12-02",
         110,
         2,
         2,
         1,
         300
        ],
        [
         "2024-12-03",
         111,
         1,
         5,
         3,
         900
        ],
        [
         "2024-12-04",
         112,
         3,
         1,
         2,
         500
        ],
        [
         "2024-12-05",
         113,
         3,
         4,
         4,
         1200
        ],
        [
         "2024-12-05",
         114,
         3,
         4,
         2,
         400
        ],
        [
         "2024-12-05",
         115,
         3,
         4,
         1,
         300
        ],
        [
         "2024-12-01",
         101,
         1,
         4,
         2,
         500
        ],
        [
         "2024-12-01",
         102,
         1,
         4,
         1,
         300
        ],
        [
         "2024-12-02",
         103,
         2,
         4,
         3,
         900
        ],
        [
         "2024-12-02",
         104,
         2,
         4,
         1,
         400
        ],
        [
         "2024-12-03",
         105,
         1,
         4,
         2,
         600
        ],
        [
         "2024-12-03",
         106,
         1,
         4,
         3,
         800
        ],
        [
         "2024-12-04",
         107,
         3,
         4,
         1,
         200
        ],
        [
         "2024-12-04",
         108,
         3,
         4,
         2,
         500
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "store_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "sale",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "order_value",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Schema for sales_data\n",
    "sales_data_schema = StructType([\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"store_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"sale\", IntegerType(), True),\n",
    "    StructField(\"order_value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for sales_data (Convert string dates to date objects)\n",
    "sales_data = [\n",
    "    (date(2024, 12, 1), 109, 1, 3, 2, 700),\n",
    "    (date(2024, 12, 2), 110, 2, 2, 1, 300),\n",
    "    (date(2024, 12, 3), 111, 1, 5, 3, 900),\n",
    "    (date(2024, 12, 4), 112, 3, 1, 2, 500),\n",
    "    (date(2024, 12, 5), 113, 3, 4, 4, 1200),\n",
    "    (date(2024, 12, 5), 114, 3, 4, 2, 400),\n",
    "    (date(2024, 12, 5), 115, 3, 4, 1, 300),\n",
    "    (date(2024, 12, 1), 101, 1, 4, 2, 500),\n",
    "    (date(2024, 12, 1), 102, 1, 4, 1, 300),\n",
    "    (date(2024, 12, 2), 103, 2, 4, 3, 900),\n",
    "    (date(2024, 12, 2), 104, 2, 4, 1, 400),\n",
    "    (date(2024, 12, 3), 105, 1, 4, 2, 600),\n",
    "    (date(2024, 12, 3), 106, 1, 4, 3, 800),\n",
    "    (date(2024, 12, 4), 107, 3, 4, 1, 200),\n",
    "    (date(2024, 12, 4), 108, 3, 4, 2, 500)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "sales_data_df = spark.createDataFrame(sales_data, schema=sales_data_schema)\n",
    "sales_data_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f3d51a-c999-476a-973d-ef74840505af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>orderid</th><th>custid</th><th>city</th><th>del_partner</th><th>order_time</th><th>deliver_time</th><th>predicted_time</th><th>actual_time</th></tr></thead><tbody><tr><td>1</td><td>101</td><td>Mumbai</td><td>Partner A</td><td>2024-12-18T10:00:00Z</td><td>2024-12-18T11:30:00Z</td><td>60</td><td>90.0</td></tr><tr><td>2</td><td>102</td><td>Delhi</td><td>Partner A</td><td>2024-12-18T09:00:00Z</td><td>2024-12-18T10:00:00Z</td><td>45</td><td>60.0</td></tr><tr><td>4</td><td>104</td><td>Mumbai</td><td>Partner A</td><td>2024-12-18T14:00:00Z</td><td>2024-12-18T14:50:00Z</td><td>45</td><td>50.0</td></tr><tr><td>9</td><td>109</td><td>Chennai</td><td>Partner C</td><td>2024-12-18T07:00:00Z</td><td>2024-12-18T07:40:00Z</td><td>30</td><td>40.0</td></tr><tr><td>10</td><td>110</td><td>Mumbai</td><td>Partner C</td><td>2024-12-18T12:00:00Z</td><td>2024-12-18T13:00:00Z</td><td>50</td><td>60.0</td></tr><tr><td>11</td><td>111</td><td>Delhi</td><td>Partner C</td><td>2024-12-18T09:00:00Z</td><td>2024-12-18T09:35:00Z</td><td>30</td><td>35.0</td></tr><tr><td>12</td><td>112</td><td>Hyderabad</td><td>Partner C</td><td>2024-12-18T16:00:00Z</td><td>2024-12-18T16:45:00Z</td><td>30</td><td>45.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         101,
         "Mumbai",
         "Partner A",
         "2024-12-18T10:00:00Z",
         "2024-12-18T11:30:00Z",
         60,
         90.0
        ],
        [
         2,
         102,
         "Delhi",
         "Partner A",
         "2024-12-18T09:00:00Z",
         "2024-12-18T10:00:00Z",
         45,
         60.0
        ],
        [
         4,
         104,
         "Mumbai",
         "Partner A",
         "2024-12-18T14:00:00Z",
         "2024-12-18T14:50:00Z",
         45,
         50.0
        ],
        [
         9,
         109,
         "Chennai",
         "Partner C",
         "2024-12-18T07:00:00Z",
         "2024-12-18T07:40:00Z",
         30,
         40.0
        ],
        [
         10,
         110,
         "Mumbai",
         "Partner C",
         "2024-12-18T12:00:00Z",
         "2024-12-18T13:00:00Z",
         50,
         60.0
        ],
        [
         11,
         111,
         "Delhi",
         "Partner C",
         "2024-12-18T09:00:00Z",
         "2024-12-18T09:35:00Z",
         30,
         35.0
        ],
        [
         12,
         112,
         "Hyderabad",
         "Partner C",
         "2024-12-18T16:00:00Z",
         "2024-12-18T16:45:00Z",
         30,
         45.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "orderid",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "custid",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "del_partner",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "deliver_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "predicted_time",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "actual_time",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Calculate the actual delivery time in minutes and filter where it exceeds the predicted_time\n",
    "result_df =swiggy_orders_df.withColumn(\"actual_time\", \n",
    "        (col(\"deliver_time\").cast(\"long\") - col(\"order_time\").cast(\"long\")) / 60\n",
    "    ).filter(col(\"actual_time\") > col(\"predicted_time\"))\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0685579d-ffc8-4f54-accf-6a7b6c34cc24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>del_partner</th><th>exceeded_count</th></tr></thead><tbody><tr><td>Partner A</td><td>3</td></tr><tr><td>Partner C</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Partner A",
         3
        ],
        [
         "Partner C",
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "del_partner",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "exceeded_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group by delivery partner and count the orders\n",
    "grouped_count_df = result_df.groupBy(\"del_partner\").count().withColumnRenamed(\"count\", \"exceeded_count\")\n",
    "grouped_count_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866e82ca-dd9b-4745-bc0a-2bc26c1aa9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>del_partner</th><th>exceeded_count</th></tr></thead><tbody><tr><td>Partner A</td><td>3</td></tr><tr><td>Partner B</td><td>0</td></tr><tr><td>Partner C</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Partner A",
         3
        ],
        [
         "Partner B",
         0
        ],
        [
         "Partner C",
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "del_partner",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "exceeded_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, sum\n",
    "\n",
    "# Calculate the actual delivery time in minutes\n",
    "result_df = (\n",
    "    swiggy_orders_df.withColumn(\n",
    "        \"actual_time\", \n",
    "        (col(\"deliver_time\").cast(\"long\") - col(\"order_time\").cast(\"long\")) / 60\n",
    "    )\n",
    ")\n",
    "\n",
    "# Group by `del_partner` and calculate the sum based on the case condition\n",
    "grouped_sum_df = (\n",
    "    result_df.groupBy(\"del_partner\")\n",
    "    .agg(\n",
    "        sum(\n",
    "            when(col(\"actual_time\") > col(\"predicted_time\"), 1).otherwise(0)\n",
    "        ).alias(\"exceeded_count\")\n",
    "    )\n",
    ")\n",
    "grouped_sum_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4761fc2-98db-4fdf-8e98-32103a2aaa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_data_df.createOrReplaceTempView('delayed_orders')\n",
    "swiggy_orders_df.createOrReplaceTempView('swiggy_orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c5b169-7135-4c64-8143-90db3cb97149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038\n",
       "\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:152)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2980)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2973)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:510)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1314)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1313)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:669)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:510)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:223)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:223)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:235)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:241)\n",
       "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
       "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
       "\tat scala.collection.immutable.List.map(List.scala:305)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:241)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:246)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:347)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:246)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:153)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:326)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:325)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:325)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:322)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:295)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2973)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2967)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:327)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:327)\n",
       "\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n",
       "\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n",
       "\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:324)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:307)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:411)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:411)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:411)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:270)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:444)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:437)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:350)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:437)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:369)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:262)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:262)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:422)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:421)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:262)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:458)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:613)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:144)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:613)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1179)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:612)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:608)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:608)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:237)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:130)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1187)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1187)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:959)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:947)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:982)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:301)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:401)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:423)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1054)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1172)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1163)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:96)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1099)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1519)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:776)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:932)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:921)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:953)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:717)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:785)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:586)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:512)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:306)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "[UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UC_NOT_ENABLED",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "56038",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:152)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2980)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2973)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:510)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1314)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1313)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:669)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:510)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:235)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:241)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.immutable.List.map(List.scala:305)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:241)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:246)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:347)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:246)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:153)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:325)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:40)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:40)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:325)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:322)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:40)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:297)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:295)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2973)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2967)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:327)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:327)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:324)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:307)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:411)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:411)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:411)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:270)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:444)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:350)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:369)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:262)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:182)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:262)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:422)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:421)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:262)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:458)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:613)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:613)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1179)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:612)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:608)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:608)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:237)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:130)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1187)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1187)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:959)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:947)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:982)\n\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:301)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:401)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:423)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:418)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1054)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1172)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1163)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:96)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1099)\n\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1519)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:776)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:932)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:921)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:953)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:717)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:785)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:586)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:512)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:306)\n\tat java.lang.Thread.run(Thread.java:750)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select a.del_partenr. isnull(delayed_oredrs,0) as delayed_orders from (select distinct del_partner from swiggy_orders) a left join (select del_operatoe, count(*) as delayed_oreds from swiggy_orders where datediff(minutr, order_time, deliver_time )>predicted_time group by del_partner ) b on a.del_partenr=b.del_partner\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2082578124513917,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Swiggy First Round SQL Interview Questions and Answers | SQL For Analytics",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
