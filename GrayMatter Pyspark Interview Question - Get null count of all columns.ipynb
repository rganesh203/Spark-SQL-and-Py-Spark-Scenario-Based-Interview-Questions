{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47b8b05a-bee1-42dd-b116-2a1a180ed5b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Problem Statement:\n",
    "\n",
    "Here are working with a PySpark DataFrame that contains several columns, and some of the cells in these columns contain null values. Your goal is to calculate the total number of null values for each column in the DataFrame. Specifically, you want to:\n",
    "\n",
    "Count the number of null values in each column.\n",
    "Display the result as a new DataFrame, where each column represents the count of null values for the corresponding column in the original DataFrame.\n",
    "This task mirrors an SQL query that counts null values using CASE statements and the SUM() function for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f6f10a4-15cf-4407-8ecc-0926620570ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col1</th><th>col2</th><th>col3</th></tr></thead><tbody><tr><td>1</td><td>null</td><td>ab</td></tr><tr><td>2</td><td>10</td><td>null</td></tr><tr><td>null</td><td>null</td><td>cd</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         null,
         "ab"
        ],
        [
         2,
         10,
         null
        ],
        [
         null,
         null,
         "cd"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col3",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, None, \"ab\"), (2, 10, None), (None, None, \"cd\")]\n",
    "columns = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df539b6b-4c3a-4016-a2b1-0f6d82ff8310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9ac0e3-7f7f-4851-ba34-2d1b61fe4695",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col1</th><th>col2</th><th>col3</th></tr></thead><tbody><tr><td>1</td><td>2</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         2,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col3",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select\n",
    "  sum(\n",
    "    case\n",
    "      when col1 is null then 1\n",
    "      else 0\n",
    "    end\n",
    "  ) as col1,\n",
    "  sum(\n",
    "    case\n",
    "      when col2 is null then 1\n",
    "      else 0\n",
    "    end\n",
    "  ) as col2,\n",
    "  sum(\n",
    "    case\n",
    "      when col3 is null then 1\n",
    "      else 0\n",
    "    end\n",
    "  ) as col3\n",
    "from\n",
    "  emp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af7f97c-8f12-43d3-b082-072069e6276e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col1_null_count</th><th>col2_null_count</th><th>col3_null_count</th></tr></thead><tbody><tr><td>1</td><td>2</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         2,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col1_null_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col2_null_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col3_null_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Count the number of nulls in each column\n",
    "df_null_count = df.select(\n",
    "    F.sum(F.when(F.col(\"col1\").isNull(), 1).otherwise(0)).alias(\"col1_null_count\"),\n",
    "    F.sum(F.when(F.col(\"col2\").isNull(), 1).otherwise(0)).alias(\"col2_null_count\"),\n",
    "    F.sum(F.when(F.col(\"col3\").isNull(), 1).otherwise(0)).alias(\"col3_null_count\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_null_count.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9db45aff-a65e-4db0-aebf-0a6f4e8e1d09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Explanation:\n",
    "\n",
    "F.when(F.col(\"col1\").isNull(), 1).otherwise(0): This creates a column that returns 1 if the value is null, otherwise 0.\n",
    "F.sum(...): This sums up the 1s to count the null values.\n",
    ".alias(\"col1_null_count\"): This renames the output columns to indicate the null count for each column."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3457368482930940,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GrayMatter Pyspark Interview Question - Get null count of all columns",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
